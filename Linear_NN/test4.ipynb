{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author-Vishal Burman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification Data(Fashion-MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mxnet import gluon\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gluon provides fashion-mnist class to download and load this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train=gluon.data.vision.FashionMNIST(train=True)\n",
    "mnist_test=gluon.data.vision.FashionMNIST(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of images in train_set and test_set is set to 6000 and 1000 for each category\n",
    "# Since there are 10 categories...therefore there are in total 60000 and 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_train), len(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following function convert a numerical label into corresponding text label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels=['t-shirt', 'trouser', 'pullover', 'dress', 'coat','sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int (i)] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising one of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist_train[10]\n",
    "X=X.squeeze(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21219e8b048>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data can be a performance bottleneck when model is simple and computer is fast\n",
    "# Gluon's dataloader uses multiple processes to speed up data processes\n",
    "# Note the number of workers defined doesn't work in windows...currently i am in Windows :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_workers(num_workers=4):\n",
    "    if sys.platform.startswith(\"win\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "transformer = gluon.data.vision.transforms.ToTensor()\n",
    "train_iter=gluon.data.DataLoader(mnist_train.transform_first(transformer), batch_size, shuffle=True, num_workers=get_dataloader_workers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the time to read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for X, y in train_iter:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"Load the fashion-mnist data into the memory\"\"\"\n",
    "    dataset=gluon.data.vision\n",
    "    trans=[dataset.transforms.Resize(resize)] if resize else []\n",
    "    trans.append(dataset.transforms.ToTensor())\n",
    "    trans=dataset.transforms.Compose(trans)\n",
    "    mnist_train=dataset.FashionMNIST(train=True).transform_first(trans)\n",
    "    mnist_test=dataset.FashionMNIST(train=False).transform_first(trans)\n",
    "    return (gluon.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=get_dataloader_workers())), (gluon.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=get_dataloader_workers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify image resizing works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter=load_data_fashion_mnist(32, (64, 64))\n",
    "for X, y in train_iter:\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Softmax Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, nd, gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "train_iter, test_iter=load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dataset has 10 categories and our weights will constitute 784x10 matrix and biases will constitute 1x10 vector\n",
    "# We initialize our weights from the Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs=784\n",
    "num_outputs=10\n",
    "\n",
    "W=nd.random.normal(scale=0.01, shape=(num_inputs, num_outputs))\n",
    "b=nd.zeros(num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attaching gradients to model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.attach_grad()\n",
    "b.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axes:\n",
    "# 0-> column\n",
    "# 1-> row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[5. 7. 9.]]\n",
       " <NDArray 1x3 @cpu(0)>, \n",
       " [[ 6.]\n",
       "  [15.]]\n",
       " <NDArray 2x1 @cpu(0)>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=nd.array([[1, 2, 3], [4, 5, 6]])\n",
    "X.sum(axis=0, keepdims=True), X.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we implemet the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The implementation is sloppy...not taken precautions against numerical overflow or underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp=X.exp()\n",
    "    partition=X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_exp/partition # The broadcast mechanism applies here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[0.13066927 0.05587501 0.05137373 0.16906908 0.59301287]\n",
       "  [0.16064428 0.16010803 0.45925817 0.10437898 0.11561053]]\n",
       " <NDArray 2x5 @cpu(0)>, \n",
       " [1. 1.]\n",
       " <NDArray 2 @cpu(0)>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=nd.random.normal(shape=(2, 5))\n",
    "X_prob=softmax(X)\n",
    "X_prob, X_prob.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(nd.dot(X.reshape((-1, num_inputs)), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pick function allows to select the appropriate terms from the matrix of softmax entries easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.1 0.5]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat=nd.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y=nd.array([0, 2], dtype='int32')\n",
    "nd.pick(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can implement the cross-entropy loss function with just one line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return -nd.pick(y_hat, y).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(axis=1)==y.astype('float32')).sum().asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can evaluate the accuracy for model net on the data set(accessed via data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):\n",
    "    metric=Accumulator(2)\n",
    "    for X, y in data_iter:\n",
    "        y=y.astype('float32')\n",
    "        metric.add(accuracy(net(X), y), y.size)\n",
    "    return metric[0]/metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulator is a utility class to accumulate sum over multiple numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator(object):\n",
    "    \"\"\"Sum a list of numbers over time\"\"\"\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self.data=[0.0]*n\n",
    "    \n",
    "    def add(self, *args):\n",
    "        self.data=[a+b for a,b in zip(self.data, args)]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.data=[0]*len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model's accuracy should be close to random guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0856"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(net, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, updater):\n",
    "    metric=Accumulator(3) #Train_loss_sum, train_acc_sum, num_examples\n",
    "    if isinstance(updater, gluon.Trainer):\n",
    "        updater=updater.step\n",
    "    for X, y in train_iter:\n",
    "        #Compute gradients and update paramaters\n",
    "        with autograd.record():\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat, y)\n",
    "        l.backward()\n",
    "        updater(X.shape[0])\n",
    "        metric.add(l.sum().asscalar(), accuracy(y_hat, y), y.size)\n",
    "    # Return training_loss and training_accuracy\n",
    "    return metric[0]/metric[2], metric[1]/metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, loss, num_epochs, updater ):\n",
    "    trains, test_acc=[], []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics=train_epoch(net, train_iter, loss, updater)\n",
    "        test_acc=evaluate_accuracy(net, test_iter)\n",
    "        print(train_metrics, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
