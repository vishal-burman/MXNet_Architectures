{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author-Vishal Burman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Multi-layer Perceptron from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd, gluon, autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "def data_load_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"Load the fashion-Mnist dataset\"\"\"\n",
    "    dataset=gluon.data.vision\n",
    "    trans=[dataset.transforms.Resize(resize)] if resize else []\n",
    "    trans.append(dataset.transforms.ToTensor())\n",
    "    trans=dataset.transforms.Compose(trans)\n",
    "    mnist_train=dataset.FashionMNIST(train=True).transform_first(trans)\n",
    "    mnist_test=dataset.FashionMNIST(train=False).transform_first(trans)\n",
    "    return ((gluon.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)), (gluon.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter=data_load_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hidden=784, 10, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1=nd.random.normal(scale=0.01, shape=(num_inputs, num_hidden))\n",
    "b1=nd.zeros(num_hidden)\n",
    "W2=nd.random.normal(scale=0.01, shape=(num_hidden, num_outputs))\n",
    "b2=nd.zeros(num_outputs)\n",
    "params=[W1, b1, W2, b2]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the maximum function directly to see how ReLU works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return nd.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X=X.reshape((-1, num_inputs))\n",
    "    H=relu(nd.dot(X, W1)+b1)\n",
    "    return nd.dot(H, W2)+b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param[:]=param-lr*param.grad/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc=mx.metric.Accuracy()\n",
    "    for i , (data, label) in enumerate(data_iterator):\n",
    "        output=net(data)\n",
    "        predictions=nd.argmax(output, axis=1) # the rowwise\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr=10, 0.5\n",
    "updater=lambda batch_size: sgd(params, lr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss: 0.7927206073125204, Train_acc 0.7898833333333334, Test_acc 0.7879\n",
      "Epoch 2. Loss: 0.4943413875579834, Train_acc 0.7704, Test_acc 0.7736\n",
      "Epoch 3. Loss: 0.4252540446917216, Train_acc 0.8489833333333333, Test_acc 0.8459\n",
      "Epoch 4. Loss: 0.3935036236445109, Train_acc 0.8423166666666667, Test_acc 0.8389\n",
      "Epoch 5. Loss: 0.3705659663836161, Train_acc 0.86105, Test_acc 0.858\n",
      "Epoch 6. Loss: 0.3490981467247009, Train_acc 0.8602166666666666, Test_acc 0.8515\n",
      "Epoch 7. Loss: 0.33836647119522095, Train_acc 0.8621666666666666, Test_acc 0.8556\n",
      "Epoch 8. Loss: 0.3278519490559896, Train_acc 0.8584, Test_acc 0.8492\n",
      "Epoch 9. Loss: 0.31651445353825886, Train_acc 0.86685, Test_acc 0.8587\n",
      "Epoch 10. Loss: 0.3083568675041199, Train_acc 0.8804, Test_acc 0.8684\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    cumulative_loss=0\n",
    "    for X, y in train_iter:\n",
    "        with autograd.record():\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat, y)\n",
    "        l.backward()\n",
    "        updater(X.shape[0])\n",
    "        cumulative_loss+=nd.sum(l).asscalar()\n",
    "    test_accuracy=evaluate_accuracy(test_iter, net)\n",
    "    train_accuracy=evaluate_accuracy(train_iter, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (epoch, cumulative_loss/60000, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
